# -*- coding: utf-8 -*-
"""HW2_BERT_112453004_李卓儀_2305

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PsSSrA9saN6IyxtdZZRrbXjO3XTERksR

# 作業 2: 牛逼！情緒分析

資料集: [Learning Word Vectors for Sentiment Analysis](https://aclanthology.org/P11-1015.pdf)

程式碼參考自: [huggingface](https://huggingface.co/)

> **資料集說明**

Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.

>**本次介紹模型為BERT**

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://github.com/google-research/bert)

程式碼參考自: [huggingface](https://huggingface.co/)

![](https://i.imgur.com/spiKPbQ.png)

[可搭配李宏毅大大的說明影片食用](https://youtu.be/UYPa347-DdE)

**訓練一個 BERT 分類模型，輸入是一句話，辨識出這句話的情緒傾向。**

### 資料集下載

- 資料集說明 :
  - text: a string feature.
  - label: a classification label, with possible values including neg (0), pos (1).

### 請注意以下 ***TODO*** 區塊是需要改寫之處
* TODO
> ```
> ##########
> # todo #
> ##########
> ```

## 安裝與載入所需套件
"""

!pip install datasets transformers

from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
import torch
import torch.nn.functional as Fun
import transformers
import matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd
import time
import warnings
warnings.filterwarnings('ignore') # setting ignore as a parameter

"""## 一些模型會用到的小函數

* TODO1: 完成 get_pred()
> 從logits的dimension=1去取得結果中數值最高者當做預測結果
* TODO2: 完成 cal_metrics()
> 透過將 tensor 轉為 numpy，可使用 sklearn 的套件算出 acc, f1_score, recall 及 precision
"""

# from jax._src.numpy.reductions import average
# get predict result
def get_pred(logits):
  '''
  Parameter
  ---------
  logits: torch.tensor, model outputs (batch_size, max_length, vocab_size)
  ---------
  '''
  return logits.argmax(dim=1)

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
from sklearn import metrics
def cal_metrics(pred, ans, method):
  pred = pred.cpu().numpy()
  ans = ans.cpu().numpy()

  accuracy = accuracy_score(ans, pred)
  # print("準確率：", accuracy)

  f1 = f1_score(ans, pred, average=method)
  # print("F1分數：", f1)

  recall = recall_score(ans, pred, average=method)
  # print("召回率：", recall)

  precision = precision_score(ans, pred, average=method)
  # print("精確率：", precision)

  return accuracy, f1, recall, precision

# save model to path
def save_checkpoint(save_path, model):
  if save_path == None:
      return
  torch.save(model.state_dict(), save_path)
  print(f'Model saved to ==> {save_path}')

# load model from path
def load_checkpoint(load_path, model, device):
  if load_path==None:
      return
  state_dict = torch.load(load_path, map_location=device)
  print(f'Model loaded from <== {load_path}')

  model.load_state_dict(state_dict)
  return model

"""## 載入資料

- 這個資料集有分成 train, test, unsupervised

- 這次我們只使用 train 及 test 資料，且我們需要將原資料重新進行分割。

- 將兩份資料合併後切割成 3:1:1 或是 8:1:1 的 train/val/test 資料集。
"""

from datasets import load_dataset

dataset = load_dataset("imdb")

"""看一下資料格式長怎樣"""

dataset

dataset['train'][0]

"""* TODO3
  >把資料拿出來後 ，將 train 及 test 合併，重新切割後，儲存下來。
"""

import pandas as pd

train_df = pd.DataFrame(dataset['train'])
test_df = pd.DataFrame(dataset['test'])

all_df = pd.concat([train_df,test_df])
all_df.head()

"""可以看一下兩個類別分布的比例"""

all_df.label.value_counts() / len(all_df)

from sklearn.model_selection import train_test_split

train_df, temp_data = train_test_split(all_df, random_state=1111, train_size=0.8)
dev_df, test_df = train_test_split(temp_data, random_state=1111, train_size=0.5)
print('# of train_df:', len(train_df))
print('# of dev_df:', len(dev_df))
print('# of test_df data:', len(test_df))

# save data
train_df.to_csv('./train.tsv', sep='\t', index=False)
dev_df.to_csv('./val.tsv', sep='\t', index=False)
test_df.to_csv('./test.tsv', sep='\t', index=False)

"""### 自定義 Dataset，將 tokenzie 的步驟放進去

* TODO4: 完成 tokenize()
> 我們會需要拿到該句話的 input_ids、attenntion_mask 及 token_type_ids。
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import torch
import torch.nn.functional as Fun

# Using Dataset to build DataLoader
class CustomDataset(Dataset):
  def __init__(self, mode, df, specify, args):
    assert mode in ["train", "val", "test"]  # 一般會切三份
    self.mode = mode
    self.df = df
    self.specify = specify # specify column of data (the column U use for predict)
    if self.mode != 'test':
      self.label = df['label']
    self.tokenizer = AutoTokenizer.from_pretrained(args["config"])
    self.max_len = args["max_len"]
    self.num_class = args["num_class"]

  def __len__(self):
    return len(self.df)

  # transform label to one_hot label (if num_class > 2)
  def one_hot_label(self, label):
    return Fun.one_hot(torch.tensor(label), num_classes = self.num_class)

  # transform text to its number
  def tokenize(self, input_text):
    # tokenize the inputs and labels
    # data = {}
    inputs = self.tokenizer.encode_plus(
            input_text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_token_type_ids=True
          )


    # data["input_ids"] = torch.tensor(inputs.input_ids, dtype=torch.long)
    # data["attention_mask"] = torch.tensor(inputs.attention_mask, dtype=torch.long)
    # return inputs["input_ids"], inputs["attention_mask"], inputs["token_type_ids"]
    return torch.tensor(inputs.input_ids, dtype=torch.long), torch.tensor(inputs.attention_mask, dtype=torch.long), torch.tensor(inputs.token_type_ids, dtype=torch.long)

  # get single data
  def __getitem__(self, index):
    sentence = str(self.df[self.specify][index])
    ids, mask, token_type_ids = self.tokenize(sentence)
    if self.mode == "test":
        return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
            torch.tensor(token_type_ids, dtype=torch.long)
    else:
        if self.num_class > 2:
          return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
            torch.tensor(token_type_ids, dtype=torch.long), self.one_hot_label(self.label[index])
        else:
          return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
            torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(self.label[index], dtype=torch.long)

"""## 建立模型

*   自己重新寫分類模型 (不可直接抄襲參考來源) ([參考來源](https://github.com/huggingface/transformers/blob/v4.39.1/src/transformers/models/bert/modeling_bert.py#L1519))
*   模型訓練與評估的程式碼大致上相同，差別在於「模型是否繼續進行梯度下降」，以及「模型參數是否繼續訓練」

* TODO5: 完成 BertClassifier
> - 在初始化的地方加上dropout, linear layer（等於一層NN），其維度為類別數量；
> - 在forward function中把輸入值放進對應層數（bert -> dropout -> classifier）；
    
    > 請注意我們只取用 bert 輸出的 sentence representation 去做分類
"""

# BERT Model
from torch import nn
class BertClassifier(BertPreTrainedModel):
  def __init__(self, config, args):
    super(BertClassifier, self).__init__(config)
    self.bert = BertModel(config)
    self.dropout = nn.Dropout(parameters["dropout"])
    self.classifier = nn.Linear(768, parameters["num_class"]) #bert-base-uncased
    self.relu = nn.ReLU()
    self.init_weights()

  # forward function, data in model will do this
  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,
              head_mask=None, inputs_embeds=None, labels=None, output_attentions=None,
              output_hidden_states=None, return_dict=None):
    _, pooled_output = self.bert(input_ids=input_ids, \
              token_type_ids=token_type_ids, \
              attention_mask=attention_mask, \
              return_dict=False)
    dropout_output = self.dropout(pooled_output)
    linear_output = self.classifier(dropout_output)
    return linear_output

"""這是已經寫好的evaluate()，train()會跟他很像

(# 在評估模型的時候，不需要梯度下降)
"""

# evaluate dataloader
def evaluate(model, data_loader, device):
  val_loss, val_acc, val_f1, val_rec, val_prec = 0.0, 0.0, 0.0, 0.0, 0.0
  step_count = 0
  loss_fct = torch.nn.CrossEntropyLoss()
  model.eval()
  with torch.no_grad():
    for data in data_loader:
      ids, masks, token_type_ids, labels = [t.to(device) for t in data]

      logits = model(input_ids = ids,
              token_type_ids = token_type_ids,
              attention_mask = masks)
      acc, f1, rec, prec = cal_metrics(get_pred(logits), labels, 'macro')
      loss = loss_fct(logits, labels)

      val_loss += loss.item()
      val_acc += acc
      val_f1 += f1
      val_rec += rec
      val_prec += prec
      step_count+=1

    val_loss = val_loss / step_count
    val_acc = val_acc / step_count
    val_f1 = val_f1 / step_count
    val_rec = val_rec / step_count
    val_prec = val_prec / step_count

  return val_loss, val_acc, val_f1, val_rec, val_prec

"""## 開始訓練

### 定義你的 Hyperparameters

* 如果電腦的記憶體不夠可以試著減少 batch_size
* 因為我們採用現有的模型去 fine-tune，所以一般不需要設太多 epochs
* config 就是我們所使用的現有模型，可以自己找適合的做替換
* 這份 work 是做二分類，所以 num_class 為 2
* 如果你的模型 overfit 了，可以把 dropout 調高
* 可以試著調高或調低 learning_rate，這會影響他的學習速度（跨步的大小）
* 你應該先檢閱你的資料再來決定 max_len （但 BERT 最大只吃到 512）
"""

from datetime import datetime
parameters = {
    "num_class": 2, #二分類
    "time": str(datetime.now()).replace(" ", "_"),
    # Hyperparameters
    "model_name": 'BERT',
    "config": 'bert-base-uncased',
    "learning_rate": 1e-5,
    "epochs": 3,
    "max_len": 450,
    "batch_size": 16,
    "dropout": 0.3,
}

"""### 載入資料

- 讀入資料並傳入自訂的 Dataset 以自訂資料格式

- 之後傳入 DataLoader 以利後續訓練進行（將資料批次化以免記憶體爆掉 ）

    (# 你可以決定要 sample 部分資料還是全部都丟進去)
"""

import transformers
import pandas as pd

# load training data
train_df = pd.read_csv('./train.tsv', sep = '\t').sample(4000).reset_index(drop=True)
train_dataset = CustomDataset('train', train_df, 'text', parameters)
train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)

# load validation data
val_df = pd.read_csv('./val.tsv', sep = '\t').sample(500).reset_index(drop=True)
val_dataset = CustomDataset('val', val_df, 'text', parameters)
val_loader = DataLoader(val_dataset, batch_size=parameters['batch_size'], shuffle=True)

"""### 初始化模型
*   載入模型（這邊會使用已經訓練過的模型，Fine-tune 我們的資料集）
*   定義 Optimization
  *   通常用 Adam 就可以了，你也可以換SGD之類的試看看
  *   可以自己看需不需要加 scheduler（可以自己寫一個 function，也可以直接套用現有的function）
  
  ［請記得 pytorch 中是以 step 去計算，想要用 epoch 去訂定需自行換算 ］



"""

transformers.logging.set_verbosity_error() # close the warning message

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertClassifier.from_pretrained(parameters['config'], parameters).to(device)
loss_fct = torch.nn.CrossEntropyLoss() # we use cross entrophy loss

## You can custom your optimizer (e.g. SGD .etc) ##
# we use Adam here
optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], betas=(0.9, 0.999), eps=1e-9)

## You also can add your custom scheduler ##
# num_train_steps = len(train_loader) * parameters['epochs]
# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_train_steps), num_training_steps=num_train_steps, num_cycles=1)

"""* 因為是做分類任務，所以這裡用 CrossEntrophyLoss
* 由於在 pytorch 中，CrossEntrophyLoss 吃的 input 是未經 softmax 的值，所以在模型中不必加入 softmax
* 但若在後期想取得各類別實際機率就要經過 softmax 轉換
* logits 的數值不等於機率 ！！但一般而言，logits 中較大者會與經過 softmax 轉換後的結果一致

* TODO6: 完成訓練
   >可參照 evaluate() 並稍作調整以完成訓練
"""

# Start training
import time
metrics = ['loss', 'acc', 'f1', 'rec', 'prec']
mode = ['train_', 'val_']
record = {s+m :[] for s in mode for m in metrics}

for epoch in range(parameters["epochs"]):

    st_time = time.time()
    train_loss, train_acc, train_f1, train_rec, train_prec = 0.0, 0.0, 0.0, 0.0, 0.0
    step_count = 0

    for data in train_loader:
      ids, masks, token_type_ids, labels = [t.to(device) for t in data]
      logits = model(input_ids = ids,
              token_type_ids = token_type_ids,
              attention_mask = masks,
              return_dict=False)

      acc, f1, rec, prec = cal_metrics(get_pred(logits), labels, 'macro')
      loss = loss_fct(logits, labels)

      model.zero_grad()
      loss.backward()
      optimizer.step()

      train_loss += loss.item()
      train_acc += acc
      train_f1 += f1
      train_rec += rec
      train_prec += prec
      step_count += 1


    # evaluate the model performace on val data after finishing an epoch training
    val_loss, val_acc, val_f1, val_rec, val_prec = evaluate(model, val_loader, device)

    train_loss = train_loss / step_count
    train_acc = train_acc / step_count
    train_f1 = train_f1 / step_count
    train_rec = train_rec / step_count
    train_prec = train_prec / step_count

    print('[epoch %d] cost time: %.4f s'%(epoch + 1, time.time() - st_time))
    print('         loss     acc     f1      rec    prec')
    print('train | %.4f, %.4f, %.4f, %.4f, %.4f'%(train_loss, train_acc, train_f1, train_rec, train_prec))
    print('val  | %.4f, %.4f, %.4f, %.4f, %.4f\n'%(val_loss, val_acc, val_f1, val_rec, val_prec))

    # record training metrics of each training epoch
    record['train_loss'].append(train_loss)
    record['train_acc'].append(train_acc)
    record['train_f1'].append(train_f1)
    record['train_rec'].append(train_rec)
    record['train_prec'].append(train_prec)

    record['val_loss'].append(val_loss)
    record['val_acc'].append(val_acc)
    record['val_f1'].append(val_f1)
    record['val_rec'].append(val_rec)
    record['val_prec'].append(val_prec)

# save model
save_checkpoint('./bert.pt' , model)

"""### 畫圖"""

# draw learning curve
EPOCHS = 3
import matplotlib.pyplot as plt
def draw_pics(record, name, img_save=False, show=False):
    x_ticks = range(1, EPOCHS+1)

    plt.figure(figsize=(6, 3))

    plt.plot(x_ticks, record['train_'+name], '-o', color='lightskyblue',
             markeredgecolor="teal", markersize=3, markeredgewidth=1, label = 'Train')
    plt.plot(x_ticks, record['val_'+name], '-o', color='pink',
             markeredgecolor="salmon", markersize=3, markeredgewidth=1, label = 'Val')
    plt.grid(color='lightgray', linestyle='--', linewidth=1)

    plt.title('Model', fontsize=14)
    plt.ylabel(name, fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.xticks(x_ticks, fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend(loc='lower right' if not name.lower().endswith('loss') else 'upper right')

    if img_save:
        plt.savefig(name+'.png', transparent=False, dpi=300)
    if show:
        plt.show()

    plt.close()

draw_pics(record, 'loss', img_save=False, show=True)

draw_pics(record, 'acc', img_save=False, show=True)

draw_pics(record, 'f1', img_save=False, show=True)

draw_pics(record, 'rec', img_save=False, show=True)

draw_pics(record, 'prec', img_save=False, show=True)

"""## 預測結果

預測單筆（跟評估的程式大同小異）
"""

def Softmax(x):
  return torch.exp(x) / torch.exp(x).sum()
# label to class
def label2class(label):
  l2c = {0:'negative', 1:'positive'}
  return l2c[label]

"""* TODO7: 完成 predict_one()"""

# predict single sentence, return each-class's probability and predicted class
def predict_one(query, model):
  tokenizer = AutoTokenizer.from_pretrained(parameters['config'])
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  model.eval()
  with torch.no_grad():
    inputs = tokenizer.encode_plus(
            query,
            None,
            add_special_tokens=True,
            max_length=parameters.get("max_len"),
            padding="max_length",
            truncation=True,
            return_token_type_ids=True
          )
    query_ids = torch.tensor([inputs.input_ids], dtype=torch.long).to(device)
    attention_mask = torch.tensor([inputs.attention_mask], dtype=torch.long).to(device)
    token_type_ids = torch.tensor([inputs.token_type_ids], dtype=torch.long).to(device)

    # forward pass
    logits = model(query_ids, attention_mask, token_type_ids)
    probs = Softmax(logits) # get each class-probs
    label_index = torch.argmax(probs[0], dim=0)
    pred = label_index.item()

  return probs, pred

# you can load model from existing result
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
init_model = BertClassifier.from_pretrained(parameters['config'], parameters) # build an initial model
model = load_checkpoint('./bert.pt', init_model, device).to(device) # and load the weight of model from specify file

# Commented out IPython magic to ensure Python compatibility.
# %%time
# probs, pred = predict_one("This movie doesn't attract me", model)
# print(label2class(pred))

"""你也可以像 evaluate function一樣，把它寫成 dataloader 的形式"""

# predict dataloader
def predict(data_loader, model):

  tokenizer = AutoTokenizer.from_pretrained(parameters['config'])
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  total_probs, total_pred = [], []
  model.eval()
  with torch.no_grad():
    for data in data_loader:
      input_ids, attention_mask, \
      token_type_ids = [t.to(device) for t in data]

      # forward pass
      logits = model(input_ids, attention_mask, token_type_ids)
      probs = Softmax(logits) # get each class-probs
      label_index = torch.argmax(probs[0], dim=0)
      pred = label_index.item()

      total_probs.append(probs)
      total_pred.append(pred)

  return total_probs, total_pred

# load testing data
test_df = pd.read_csv('./test.tsv', sep = '\t')
test_dataset = CustomDataset('test', test_df, 'text', parameters)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

total_probs, total_pred = predict(test_loader, model)
res = test_df.copy()
# add predict class of origin file
res['pred'] = total_pred

# save result
res.to_csv('./result.tsv', sep='\t', index=False)

res.head(5)

correct = 0
for idx, pred in enumerate(res['pred']):
  if pred == res['label'][idx]:
    correct += 1
print('test accuracy = %.4f'%(correct/len(test_df)))